Chapter-1 Transformer Models:

Limitations of llms:
  Hallucinations:Generating missinformation
  Lack of understanding: Operate purely on statistical patterns
  Computational resources: Require significant computational resources.

Transformer Pipelines:
  sentiment-analysis
  zero-shot-classification
  text-generation
  fill-mask
  ner (named entity recognition)
  question-answering
  summarization
  translation
  feature-extraction
  Image:
    image-to-text
    object-detection
    image-classification
  Audio:
    automatic-speech-recognition
    audio-classification
    text-to-speech
  image-text-to-text (respond to an image based on a text)

Steps:
  Text pre-processing
  Inputs are passed to the model
  Output post-processing for return

Encoder:
  Bi-directional : context from the left and the right of the word
  NLU: Natural Language Understanding
  Examples: BERT RoBERTa ALBERT

Decorder:
  Uni-directional: Access to their left or right context, Alway s remember the first word
  NLG: Natural Language Generation
  Examples: GPT-2 GPT Neo 

Encoder-Only models: Tasks that require understanding, sentence classification, NER
Decorer-Only models: Generative tasks, text generation.
Encoder-Decoder models or sequence-to-sequence models: Generative tasks that require input, translation
summarization.

Masked language modeling : BERT, randomly masks some tokens in the input and trains the model to predict the tokens based on context (bidirectional)
Causal Language Modeling : GPT, predicts the next token based on all previous tokens.

Softmax : Probability distribution

Two phases of LLM training:
  Pretraining: The model learns to predict next token 
  Instruction Tuning: The model is fine-tuned to follow instructions.

LSH Attention : Greatest softmax scores are more important, closer query key combinations would be in same bucket. several hash functions are used and averaged together
Local Attention: A window of left and right of the token
Axial Positional Encoding: Divide positional embeddings

Context Length: Maximum number of tokens that LLM can process at once.(model's working memory)
Careful design of prompt guide the generation of the LLM toward the desired output.

Two-Phase Inference Process:
  Prefill Phase: Tokenization, embedding conversion and inital processing. Computationally intensive, reading and understanding an entire paragraph before starting to write a response.
  Decode Phase: Autoregressive text generation, attention computation, prob calculation, token select. continuation check. Memory intensive, keep track of all previously generated tokens and their relationships.

Token Selection:
  Raw Logits (initial scores), temperature scaling(random or deterministic choices), sampling(thresholding), top-k filtering

Managin Repetition:
  Presence Penalty: Fixed penalty to any token that has appeared before
  Frequency Penalty: Scaling penalty that increases based on how often a token has been used.
Beam Search:
  Looking for multiple possible paths for each candidate compute probs for the next token keep only the most promising combinations of sequences and their next tokens. select highest overall probability. 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Chapter-2 Using Transformers:

Tokenization: AutoTokenizer class' from_pretrained() method. Numerical value for inputs
Usage: 
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

Model : AutoModel class' from_pretrained() method. Embeddings create vectors for tokens and manipulate.
Usage:
from transformers import AutoModel
model = AutoModel.from_pretrained(checkpoint)
pre_postprocessed = model(**tokenized_input)

Post Processing: Torch's nn.softmax() method. Creating probabilities from embeddings.
Usage:
import torch
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

